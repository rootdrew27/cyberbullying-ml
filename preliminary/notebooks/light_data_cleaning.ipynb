{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootdrew27/cyberbullying-ml/blob/main/light_data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nbQiHKDzHgz1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\rooty\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\rooty\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\rooty\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from langdetect import detect, LangDetectException\n",
        "import contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxZybHwJNI5T",
        "outputId": "6b1de6a4-c860-45a2-db10-b0937856b7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path_1 = '../data/original/cyberbullying_tweets.csv'\n",
        "data_path_2 = '../data/original/hatespeech_tweets.csv'\n",
        "data_path_3 = '../data/original/original/TweetBLM (1).csv'\n",
        "\n",
        "#df1 = pd.read_csv(data_path_1, encoding='utf-8')\n",
        "df2 = pd.read_csv(data_path_2, encoding='utf-8')\n",
        "# df3 = pd.read_csv(data_path_3, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To Open csv files with utf-8 encoding: https://superuser.com/questions/280603/how-to-set-character-encoding-when-opening-a-csv-file-in-excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I basically live inside the Matrix. http://t.co/PVQqXB5hUU'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1.iloc[3128]['tweet_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "collapsed": true,
        "id": "pwkDITBhPsAA",
        "outputId": "350c4b5a-0736-4df3-be05-e02f9ad9cdc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'jokes about rape are like calling people gay okay if you are a liberal it is even better if you are a liberal and give it the one two punch of being a joke about prison rape it is okay though because it offends people you do not like which is the defining trait of morality'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1.sample()['tweet_text'].iloc[0] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'#RayRice is a bitch. #JustSaying'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2['tweet'].iloc[757]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RayRice is a bitch JustSaying'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove_short_tweetsremove_excess_spaces(remove_space_at_beginning_or_end(remove_punctuation(decontract(remove_html_entities(remove_rt(remove_mentions(remove_links(df2['tweet'].iloc[757]))))))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wCGV7A3bu0gK"
      },
      "outputs": [],
      "source": [
        "# tweets with \\r\\n were not properly read (or written) thus we will remove all tweets with '\\r\\n' as they are likely\n",
        "# df1 = df.copy()\n",
        "# df1 = df1[df1.tweet_text.str.contains('\\r\\n') == False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjcYZc_sv254"
      },
      "outputs": [],
      "source": [
        "# note that tweets can technically contain more than 280 characters as links will always count as 22 characters\n",
        "# source: https://smallbusiness.chron.com/doesnt-count-characters-twitter-77095.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_non_english(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except LangDetectException:\n",
        "        lang = \"unknown\"\n",
        "    return text if lang == \"en\" else \"\"\n",
        "\n",
        "def remove_links(text):\n",
        "    return re.sub(r'(?:http[s]?:\\/\\/)\\S*|(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S*', r' ', text)\n",
        "\n",
        "def remove_html_entities(text):\n",
        "    return re.sub(r'&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-fA-F]{1,6});|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-fA-F]{1,6})', r' ', text)\n",
        "\n",
        "def remove_mentions(text):\n",
        "    return re.sub(r'@\\w+', '', text)\n",
        "\n",
        "def remove_rt(text):\n",
        "    return re.sub(r'\\srt\\s|\\sRT\\s', r' ',text)\n",
        "\n",
        "def decontract(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', r'', text)\n",
        "\n",
        "def remove_digits(text):\n",
        "    return re.sub(r'\\d', r' ', text)\n",
        "\n",
        "def remove_short_tweets(text):\n",
        "    return text if len(text.split()) > 2 else \"\"\n",
        "\n",
        "def remove_space_at_beginning_or_end(text):\n",
        "    return text.strip() \n",
        "\n",
        "def remove_excess_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "def lemmatize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(t) for t in tokens])\n",
        "\n",
        "def stem(text):\n",
        "    tokens = word_tokenize(text)    \n",
        "    return ' '.join([stemmer.stem(t) for t in tokens])\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    return ' '.join([w for w in word_tokenize(text) if w not in STOP_WORDS])   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ifS9wfCRHoyj"
      },
      "outputs": [],
      "source": [
        "# data cleaning functions\n",
        "\n",
        "def standardize_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_non_english(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except LangDetectException:\n",
        "        lang = \"unknown\"\n",
        "    return text if lang == \"en\" else \"\"\n",
        "\n",
        "def remove_links(text):\n",
        "    url_pattern = re.compile(r'(?:http[s]?:\\/\\/)\\S*|(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S*') #remove url shorteners\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def remove_html_entities(text):\n",
        "    return re.sub(r'&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-fA-F]{1,6});|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-fA-F]{1,6})', r'', text)\n",
        "\n",
        "def remove_mentions(text):\n",
        "    return re.sub(r'@\\w+', '', text)\n",
        "\n",
        "def remove_rt(text):\n",
        "    return re.sub(r'\\Art', '', text) # NOTE This misses some 'rt's and thus we remove them later\n",
        "\n",
        "def decontract(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', r' ', text)\n",
        "\n",
        "def remove_digits(text):\n",
        "    return re.sub(r'\\d', r' ', text)\n",
        "\n",
        "def remove_short_tweets(text):\n",
        "    return text if len(text.split()) > 2 else \"\"\n",
        "\n",
        "def remove_space_at_beginning_or_end(text):\n",
        "    return re.sub(r'\\A\\s|\\s\\Z', r'', text) \n",
        "\n",
        "def remove_excess_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BEDC-xsf146U"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  text = standardize_text(text)\n",
        "  text = remove_links(text)\n",
        "  text = remove_mentions(text)\n",
        "  text = remove_rt(text)\n",
        "  text = remove_html_entities(text)\n",
        "  text = decontract(text)\n",
        "  #text = remove_non_english(text)\n",
        "  text = remove_punctuation(text)\n",
        "  #text = remove_digits(text)\n",
        "  text = remove_excess_spaces(text)\n",
        "  text = remove_space_at_beginning_or_end(text)\n",
        "  text = remove_short_tweets(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q35u1Xw31jmh"
      },
      "outputs": [],
      "source": [
        "# df1.tweet_text = df1.tweet_text.apply(preprocess)\n",
        "# df2.tweet = df2.tweet.apply(preprocess)\n",
        "df3.text = df3.text.apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1.tweet_text = df1.tweet_text.apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "FM7SeghZ2aWo",
        "outputId": "8d376361-ce46-4650-998a-2d30aa7d6c64"
      },
      "outputs": [],
      "source": [
        "# df1.dropna(axis=0, inplace=True) #drop rows that contain any null values\n",
        "# df1.reset_index(drop=True, inplace=True) #reset the indexes\n",
        "\n",
        "# df2.dropna(axis=0, inplace=True)\n",
        "# df2.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df3.dropna(axis=0, inplace=True)\n",
        "df3.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'there is no lds doctrine to support or explain the radical premise likely inspired by esoteric kabbalism that god evolved from humankind over the years the church has distanced itself more and more from this idea and has made efforts to prioritize its christian values'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1.sample()['tweet_text'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#result copies\n",
        "# df1_copy = df1.copy(deep=True)\n",
        "# df2_copy = df2.copy(deep=True)\n",
        "df3_copy = df3.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove the \"rt\"s missed. oops\n",
        "# df1_copy.tweet_text = df1_copy.tweet_text.apply(lambda text: re.sub(r'\\srt\\s|\\Art', r' ', text))\n",
        "# df2_copy.tweet = df2_copy.tweet.apply(lambda text: re.sub(r'\\srt\\s|\\Art', r' ', text))\n",
        "df3_copy.text = df3_copy.text.apply(lambda text : re.sub(r'\\srt\\s|\\Art', r' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tfw you and the guys are talking in discord and the server idiot drops in to talk about muslims for no reason'"
            ]
          },
          "execution_count": 404,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1_copy.sample()['tweet_text'].iloc[0] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'this monkey on my back keeps smearing poop in my hair'"
            ]
          },
          "execution_count": 405,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2_copy.sample()['tweet'].iloc[0] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JDlEikY93iWt"
      },
      "outputs": [],
      "source": [
        "# df1_copy.to_csv('../data/clean/bullying_light_clean_2.csv', index=False)\n",
        "# df2_copy.to_csv('../data/clean/hatespeech_light_clean_2.csv', index=False)\n",
        "df3_copy.to_csv('../data/clean/tweet_blm_light_clean.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIRNWiVIx50O"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPUe9L43JOv/OKPAjj62T4L",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
