{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFEYUs3fnVx4L14zxc4Q0/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootdrew27/cyberbullying-ml/blob/main/DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "\n",
        "- This notebook implements the functions to clean our cyberbullying data\n",
        "- It replaces mentions with <@> and hastags are removed. (See implementation for more details)"
      ],
      "metadata": {
        "id": "QIuNpX__nKD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!pip install nltk\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "gP6XBl4ei90d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QnyjRpqHi2mC"
      },
      "outputs": [],
      "source": [
        "# data management\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# preprocessing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from langdetect import detect, DetectorFactory, LangDetectException\n",
        "import contractions\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./cyberbullying_tweets.csv', header=0)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "C8N2PzJrjAfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert string labels to integer representation.**"
      ],
      "metadata": {
        "id": "tg9TSq9ylNcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = {\n",
        "    'not_cyberbullying':0,\n",
        "    'religion':1,\n",
        "    'age':2,\n",
        "    'gender':3,\n",
        "    'ethnicity':4,\n",
        "    'other_cyberbullying':5\n",
        "}\n",
        "df['cyberbullying_type'] = df['cyberbullying_type'].replace(class_labels).astype(int)"
      ],
      "metadata": {
        "id": "7uVB24nMjGiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning functions**"
      ],
      "metadata": {
        "id": "T8CsAArslrHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove tweets with more than 280 characters, as this is past the tweet limit and thus there must have been an issue loading the data\n",
        "def remove_bad_data(text):\n",
        "  return text if len(text) < 280 else \"\"\n",
        "\n",
        "def standardize(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Function to check if the text is in English, and return an empty string if it's not\n",
        "def remove_non_english(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except LangDetectException:\n",
        "        lang = \"unknown\"\n",
        "    return text if lang == \"en\" else \"\"\n",
        "\n",
        "# Expand contractions\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_entities(text):\n",
        "    text = re.sub(r'&[a-z]+;', r' ', text) #remove html entities\n",
        "    text = re.sub(r'https?\\://S*', r' ', text) # remove links\n",
        "    text = re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text) #remove url shorteners\n",
        "    text = re.sub(r'#\\S*', r'', text) #remove hastags\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', r'', text) #remove non-ascii characters\n",
        "    text = re.sub(r'[!$%^&*+=\\-_()[\\]\\\\;|:`~\\'\",./?<>}{]', r' ', text) #remove punctuation and special chars\n",
        "    text = re.sub(r'[0-9]', r' ', text) #remove numbers\n",
        "    text = re.sub(r'@\\S*', r'@', text) # normalize mentions\n",
        "    text = re.sub(r'\\s', r' ', text) #replace whitespace chars with a single space\n",
        "    return text\n",
        "\n",
        "# Lemmatize words\n",
        "# def lemmatize(text):\n",
        "#     words = word_tokenize(text)\n",
        "#     lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "#     return ' '.join(lemmatized_words)\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    text = \" \".join([word for word in text.split(\" \") if word not in stopWords])\n",
        "    return text\n",
        "\n",
        "def remove_excess_spaces(text):\n",
        "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
        "\n",
        "def remove_blank_chars(text):\n",
        "    return \" \".join([char for char in text if char != ''])\n",
        "\n",
        "def remove_tweets_with_few_words(text):\n",
        "    if len(text.split(\" \")) < 4:\n",
        "        text = \"\"\n",
        "    return text\n",
        "\n",
        "elo_word_re_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n",
        "\n",
        "#Naive impl of elongated word replacer\n",
        "def replace_elongated_words(text):\n",
        "    return re.sub(elo_word_re_pattern, r'\\1\\3\\4', text)\n",
        "\n",
        "def preprocess(text):\n",
        "    text = remove_bad_data(text)\n",
        "    text = remove_non_english(text)\n",
        "    text = standardize(text)\n",
        "    text = replace_elongated_words(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_entities(text)\n",
        "    #text = remove_stop_words(text)\n",
        "    text = remove_excess_spaces(text)\n",
        "    text = remove_tweets_with_few_words(text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "u4mnZDwZlKpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning functions (specified in preprocess)\n",
        "\n",
        "df.tweet_text = df.tweet_text.apply(preprocess)"
      ],
      "metadata": {
        "id": "2e3rfYd-l0Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(axis=0, inplace=True) #drop rows that contain any null values\n",
        "df.reset_index(drop=True) #reset the indexes"
      ],
      "metadata": {
        "id": "lefdgHOvmCO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}