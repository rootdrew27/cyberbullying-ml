{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from utils.params import get_topn_param_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_PATH = Path(r'C:\\Users\\rooty\\UWEC\\Research\\CyberBullyingML\\cyberbullyingml\\cyberbullying-ml\\official\\hyp_search\\results')\n",
    "DATA_PATH = Path(r'C:\\Users\\rooty\\UWEC\\Research\\CyberBullyingML\\cyberbullyingml\\cyberbullying-ml\\data\\en_only')\n",
    "DATASET_1_NAME = '48000_cyberbullying_tweets_basic_clean.csv'\n",
    "DATASET_2_NAME = 'hatespeech_tweets_basic_clean.csv'\n",
    "DATASET_3_NAME = 'onlineHarassmentDataset_basic_clean.csv'\n",
    "\n",
    "RANDOM_SEED = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(DATA_PATH / DATASET_1_NAME)\n",
    "d2 = pd.read_csv(DATA_PATH / DATASET_2_NAME)\n",
    "d3 = pd.read_csv(DATA_PATH / DATASET_3_NAME)\n",
    "\n",
    "d1.dropna(axis=0, inplace=True)\n",
    "d1.drop_duplicates(inplace=True)\n",
    "d1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "d2.dropna(axis=0, inplace=True)\n",
    "d2.drop_duplicates(inplace=True)\n",
    "d2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "d3.dropna(axis=0, inplace=True)\n",
    "d3.drop_duplicates(inplace=True)\n",
    "d3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "d1['label'] = d1['label'].apply(lambda label: 0 if label =='notcb' else 1)\n",
    "d2['class'] = d2['class'].apply(lambda label: 1 if label == 0 else 0)\n",
    "d3['Code'] = d3['Code'].apply(lambda label: 1 if label == 'H' else 0)\n",
    "\n",
    "d1.rename(columns={'tweet': 'text'}, inplace=True)\n",
    "d2.rename(columns={'tweet': 'text'}, inplace=True)\n",
    "d2.rename(columns={'class': 'label'}, inplace=True)\n",
    "d3.rename(columns={'Code': 'label'}, inplace=True)\n",
    "d3.rename(columns={'Tweet': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test  = train_test_split(d1['text'], d1['label'], random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "x_train = cv.fit_transform(x_train)\n",
    "x_test = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import array\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def _make_int_array():\n",
    "    \"\"\"Construct an array.array of a type suitable for scipy.sparse indices.\"\"\"\n",
    "    return array.array(str(\"i\"))\n",
    "\n",
    "\n",
    "class MyVectorizer(CountVectorizer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.oov_token_count = 0\n",
    "        self.iv_token_count = 0\n",
    "        \n",
    "    def _count_vocab(self, raw_documents, fixed_vocab):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        analyze = self.build_analyzer()\n",
    "        j_indices = []\n",
    "        indptr = []\n",
    "\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "        for doc in raw_documents:\n",
    "            feature_counter = {}\n",
    "            for feature in analyze(doc):\n",
    "                try:\n",
    "                    feature_idx = vocabulary[feature]                    \n",
    "                    if fixed_vocab == True: self.iv_token_count += 1\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                    \n",
    "                except KeyError:        \n",
    "                    if fixed_vocab == True: self.oov_token_count += 1\n",
    "                    # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                    continue\n",
    "\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\n",
    "                    \"empty vocabulary; perhaps the documents only contain stop words\"\n",
    "                )\n",
    "\n",
    "        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n",
    "            # if _IS_32BIT:\n",
    "            #     raise ValueError(\n",
    "            #         (\n",
    "            #             \"sparse CSR array has {} non-zero \"\n",
    "            #             \"elements and requires 64 bit indexing, \"\n",
    "            #             \"which is unsupported with 32 bit Python.\"\n",
    "            #         ).format(indptr[-1])\n",
    "            #     )\n",
    "            indices_dtype = np.int64\n",
    "\n",
    "        else:\n",
    "            indices_dtype = np.int32\n",
    "        j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "        indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "        values = np.frombuffer(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix(\n",
    "            (values, j_indices, indptr),\n",
    "            shape=(len(indptr) - 1, len(vocabulary)),\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = MyVectorizer(max_df=.9)\n",
    "x_train = v.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = v.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250373"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.iv_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7631"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.oov_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total token count = 258004\n",
      "ratio of oov to total = 0.029577060820762467\n"
     ]
    }
   ],
   "source": [
    "total = v.oov_token_count + v.iv_token_count \n",
    "ratio = v.oov_token_count / total\n",
    "\n",
    "print(f'total token count = {total}')\n",
    "print(f'ratio of oov to total = {ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train, y_test  = train_test_split(d1['text'], d1['label'], random_state=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(x_test1 == x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x_test[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43401    50\n",
       "12549    30\n",
       "32707    24\n",
       "14919    22\n",
       "10689    15\n",
       "         ..\n",
       "37513    20\n",
       "31       28\n",
       "8799     41\n",
       "5568     26\n",
       "31500     8\n",
       "Name: text, Length: 10919, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test1 = x_test1.apply(lambda doc: sum(v.transform([doc])[0].data))\n",
    "x_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250373"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221521"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse._csr import csr_matrix\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [[\"hello\", \"world\", \"hello\", \"friend\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "vocabulary = {}\n",
    "for d in docs:\n",
    "    for term in d:\n",
    "        index = vocabulary.setdefault(term, len(vocabulary))\n",
    "        indices.append(index)\n",
    "        data.append(1)\n",
    "    indptr.append(len(indices))\n",
    "\n",
    "csr_matrix((data, indices, indptr), dtype=int).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 221521 stored elements and shape (10919, 38727)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Starting run 1\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Starting run 2\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Starting run 3\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Starting run 4\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Starting run 5\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n",
      "Get OOV ratio\n"
     ]
    }
   ],
   "source": [
    "results_file = Path.cwd() / 'oov_results'\n",
    "results = []\n",
    "\n",
    "for idx, (tr_ds_name, te_ds_name, tr_df, te_df) in enumerate([('d1', 'd2', d1, d2), ('d1', 'd3', d1, d3), ('d2', 'd1', d2, d1), ('d2', 'd3', d2, d3), ('d3', 'd1', d3, d1), ('d3', 'd2', d3, d2)]):\n",
    "    print(f'Starting run {idx}')\n",
    "    for model in ['catb', 'xgb']:\n",
    "        param_results:pd.DataFrame = get_topn_param_sets(PARAMS_PATH, algo=model, dataset=tr_ds_name, n=10, sort_condition='f1_macro_mean')\n",
    "        param_sets = param_results['params']\n",
    "\n",
    "        snowballer = SnowballStemmer('english')     \n",
    "        def pp_SnowballStemmer(text):\n",
    "            return ' '.join([snowballer.stem(word) for word in text.split()])\n",
    "        \n",
    "        for i, params in enumerate(param_sets):\n",
    "            x_train, x_val, y_train, y_val = train_test_split(\n",
    "                tr_df['text'],\n",
    "                tr_df['label'],\n",
    "                test_size=0.1,\n",
    "                shuffle=True,\n",
    "                stratify=tr_df['label'],\n",
    "                random_state=RANDOM_SEED\n",
    "            )\n",
    "            x_test = te_df['text']\n",
    "            vectorizer_cls = CountVectorizer if 'Count' in params['vectorizer'] else TfidfVectorizer\n",
    "            vect_params = {}\n",
    "\n",
    "            for k in params.keys():\n",
    "                if 'vectorizer__' in k:\n",
    "                    p_name = k.split('__')[1]            \n",
    "                    if p_name == 'ngram_range':\n",
    "                        vect_params[p_name] = tuple(params[k])  \n",
    "                    elif p_name == 'preprocessor':\n",
    "                        vect_params[p_name] = pp_SnowballStemmer if isinstance(params[k], str) else None\n",
    "                    else:\n",
    "                        vect_params[p_name] = params[k]\n",
    "\n",
    "            vect1 = vectorizer_cls(**vect_params)\n",
    "            \n",
    "                    \n",
    "            print(\"Get OOV ratio\")\n",
    "\n",
    "            myv = MyVectorizer(**vect_params)\n",
    "            myv.fit_transform(x_train)\n",
    "            myv.transform(x_test)\n",
    "\n",
    "            total = myv.oov_token_count + myv.iv_token_count \n",
    "            ratio = myv.oov_token_count / total\n",
    "\n",
    "            results.append(ratio)\n",
    "\n",
    "with open('oov_ratios.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "with open('oov_ratios.json', 'r') as f:\n",
    "    print(len(json.load(f)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyberbullyingml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
