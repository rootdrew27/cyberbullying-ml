{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, ShuffleSplit, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\rooty\\\\UWEC\\\\Research\\\\CyberBullyingML\\\\venv\\\\cyberbullying-ml\\\\')\n",
    "from src.utils.results import create_results_file, append_results_to_json\n",
    "from src.utils.data import balance_train_and_test, get_OOV_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 115\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_DATA_PATH = Path(\"../data/en_only\")\n",
    "EXPERIMENTS_PATH = Path('C:\\\\Users\\\\rooty\\\\UWEC\\\\Research\\\\CyberBullyingML\\\\venv\\\\cyberbullying-ml\\\\experiments')\n",
    "RESULT_PATH = Path('C:\\\\Users\\\\rooty\\\\UWEC\\\\Research\\\\CyberBullyingML\\\\venv\\\\cyberbullying-ml\\\\experiments\\\\results\\\\exp3')\n",
    "\n",
    "TRAIN_DATA_NAME = '48000_cyberbullying_tweets_basic_clean.csv' \n",
    "TEST_DATA_NAME = 'hatespeech_tweets_basic_clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = CLEAN_DATA_PATH / TRAIN_DATA_NAME\n",
    "test_data_path = CLEAN_DATA_PATH / TEST_DATA_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Semi-Optimal Hyperparameters \n",
    "\n",
    "AUC of 0.5-0.7 (BAD), 0.7-0.8 (POOR), 0.8-0.9 (GOOD), 0.9-1.0 (EXCELLENT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# remove duplicates\n",
    "train_df = train_df.drop_duplicates()\n",
    "# remove rows with missing values\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].apply(lambda label : 0 if label == 'notcb' else 1)\n",
    "\n",
    "label2id = {'notcb': 0, 'cb': 1}\n",
    "id2label = {0: 'notcb', 1: 'cb'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_df['tweet'], train_df['label'], test_size=0.05, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballer = SnowballStemmer('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def pp_SnowballStemmer(text):\n",
    "    return ' '.join([snowballer.stem(word) for word in text.split()])\n",
    "\n",
    "def pp_PorterStemmer(text):\n",
    "    return ' '.join([porter.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\n",
    "    'vectorizer': [CountVectorizer, TfidfVectorizer],\n",
    "    'vectorizer__ngram_range': [(1,1), (1,3)],\n",
    "    'vectorizer__preprocessor': [pp_SnowballStemmer],\n",
    "    'vectorizer__max_df': [0.5, 0.75],\n",
    "    'classifier__learning_rate': [0.01, 0.1],\n",
    "    'classifier__n_estimators': [100, 500],\n",
    "    'classifier__colsample_bytree': [0.5, 0.75],\n",
    "    'classifier__max_depth': [6, 12],\n",
    "    'classifier': [XGBClassifier]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = (v for _, v in xgb_param_grid.items())\n",
    "keys = xgb_param_grid.keys()\n",
    "param_sets = [params for params in itertools.product(*iters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive weight is 0.1715\n"
     ]
    }
   ],
   "source": [
    "# calculate the scale_pos_weight\n",
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f'The positive weight is {pos_weight:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   38.4s remaining:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   38.4s remaining:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   38.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cv results for run 0 are:\n",
      "    fit_time  score_time  test_f1_macro  test_f1_weighted\n",
      "0  32.812652    0.042157       0.600689          0.707024\n",
      "1  32.807300    0.043509       0.597585          0.703347\n",
      "2  32.677444    0.046913       0.589113          0.694266\n",
      "3  32.844946    0.042316       0.597445          0.703624\n",
      "4  33.048005    0.030066       0.607920          0.716048\n",
      "\n",
      "Starting run 1...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m vect \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvect_params)\n\u001b[0;32m     22\u001b[0m classifier \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m](early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m, scale_pos_weight\u001b[38;5;241m=\u001b[39mpos_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclassifier_params)\n\u001b[1;32m---> 24\u001b[0m x_train_copy \u001b[38;5;241m=\u001b[39m \u001b[43mvect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m x_val_copy \u001b[38;5;241m=\u001b[39m vect\u001b[38;5;241m.\u001b[39mtransform(x_val)\n\u001b[0;32m     26\u001b[0m y_train_copy \u001b[38;5;241m=\u001b[39m y_train\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Research\\CyberBullyingML\\venv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Research\\CyberBullyingML\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Research\\CyberBullyingML\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Research\\CyberBullyingML\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:108\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36mpp_SnowballStemmer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpp_SnowballStemmer\u001b[39m(text):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([snowballer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()])\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpp_SnowballStemmer\u001b[39m(text):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43msnowballer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()])\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Research\\CyberBullyingML\\venv\\lib\\site-packages\\nltk\\stem\\snowball.py:1773\u001b[0m, in \u001b[0;36mEnglishStemmer.stem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m r2\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1772\u001b[0m     word \u001b[38;5;241m=\u001b[39m word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1773\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m r1\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   1775\u001b[0m         word[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__vowels\n\u001b[0;32m   1776\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m word[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwxY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m word[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__vowels\n\u001b[0;32m   1778\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m word[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__vowels\n\u001b[0;32m   1779\u001b[0m     ):\n\u001b[0;32m   1780\u001b[0m         word \u001b[38;5;241m=\u001b[39m word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_results = {'fit_time': [], 'score_time': [], 'test_f1_macro': [], 'test_f1_weighted': [], 'params': []}\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "for idx, params in enumerate(param_sets):\n",
    "    print(f'Starting run {idx}...\\n')\n",
    "\n",
    "    t = {k: v for k, v in zip(keys, params)}\n",
    "\n",
    "    vect_params = {}\n",
    "    classifier_params = {}\n",
    "\n",
    "    for k in keys:\n",
    "        if 'vectorizer__' in k:\n",
    "            p_name = k.split('__')[1]\n",
    "            vect_params[p_name] = t[k]\n",
    "        if 'classifier__' in k:\n",
    "            p_name = k.split('__')[1]\n",
    "            classifier_params[p_name] = t[k]\n",
    "\n",
    "    vect = t['vectorizer'](**vect_params)\n",
    "    classifier = t['classifier'](early_stopping_rounds=7, eval_metric='logloss', scale_pos_weight=pos_weight, **classifier_params)\n",
    "\n",
    "    x_train_copy = vect.fit_transform(x_train)\n",
    "    x_val_copy = vect.transform(x_val)\n",
    "    y_train_copy = y_train\n",
    "    y_val_copy = y_val\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    cv_results: dict = cross_validate(\n",
    "        classifier,\n",
    "        x_train_copy,\n",
    "        y_train_copy,\n",
    "        cv=skf,\n",
    "        scoring=['f1_macro', 'f1_weighted'],\n",
    "        verbose=10,\n",
    "        return_estimator=False,\n",
    "        params={'eval_set':[(x_val_copy, y_val_copy)]},\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(f'The cv results for run {idx} are:\\n{pd.DataFrame(cv_results)}\\n')\n",
    "\n",
    "    cv_results['params'] = [t for _ in range(N_SPLITS)]\n",
    "    for key, values in cv_results.items():\n",
    "        all_results[key].extend(values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv_results_xgb_d1.json', 'w+') as f:\n",
    "    json.dump(all_results, f, default=str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
